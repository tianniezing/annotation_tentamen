{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMt7ctoCp7oI"
      },
      "source": [
        "# Week 4 | Assignment 2: Evaluating & Adjudicating Annotations - Part I\n",
        "\n",
        "\n",
        "### Overview\n",
        "* You start from the annotations and guidelines that you produced last week\n",
        "* You will evaluate the annotations quantatively (\"researchers\") and qualitatively (\"annotators\")\n",
        "* Next week: make final, \"gold\" version of your annotations"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stage 1.1: Setting things up\n",
        "\n",
        "* If you were in the \"researchers\" team last week, you'll now be in the \"annotators\" team, and the other way around \n",
        "* In the rest of the notebook, sections marked \"/R\" are meant for the researchers, sections marked \"/A\" are for annotators\n",
        "\n",
        "> **IMPORTANT**: \n",
        "> * **make a copy of this notebook (_File > Save a copy in Drive_) and share it with your groupmates. One team member should submit the notebook on Nestor before the start of the next lab session.**\n",
        "> * **also include external documents / spreadsheets that you created as part of the assignment when you submit**"
      ],
      "metadata": {
        "id": "qdW_BgIxxSFS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stage 1.2/R: Inter-Annotator Agreement\n",
        "\n",
        "* How inter-annotator agreement (\"IAA\" for short) works was discussed in the last lecture. Review the [slides here](https://annotationfor-wmh5113.slack.com/files/U03BU7F0DH9/F03FGQU128G/week_04_-_iaa.pdf) and/or read chapter 6 (p. 126 onwards) from the [book](https://rug-on-worldcat-org.proxy-ub.rug.nl/search/detail/801812987?submitButton=&queryString=natural%20language%20annotation%20for%20machine%20learning&databaseList=638).\n",
        "* In the assignments (as in the lecture), we'll work with Cohen's and Fleiss' $\\kappa$ metric.\n"
      ],
      "metadata": {
        "id": "Z86MPKLWFb1q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preparing the data\n",
        "\n",
        "* On Doccano, find the projects containing the annotations created in **week 3** (part II of Assignment 1)\n",
        "* For each project, export the annotations (Datasets > Actions > Export Annotations > JSONL)\n",
        "* Unzip the downloaded files, find the files containing the actual annotations (tip: this is the file that has the project code as filename, e.g. `rd-event-1.jsonl`) and rename them with the names of the annotators (e.g. `hermoine.jsonl`, `harry.jsonl`, `ron.jsonl`)\n",
        "* In the \"Files\" pane (left-hand menu in Colab), create a new folder `annotations/` and upload the annotation files to this folder"
      ],
      "metadata": {
        "id": "mIo6Q-eCvLXx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Write the following three functions:**\n",
        "1. a function that reads a single annotation file and returns the JSON lines as a list of dictionaries*\n",
        "2. a function that groups annotations per annotator. This function should take as input a dictionary mapping each annotator to that annotator's list of annotations, and should return a dictionary mapping document IDs to a dictionary containing the annotations for that document ID per annotator. Example input:\n",
        "```python\n",
        "{\n",
        "    \"hermione\": [annotation_1, annotation_2, annotation_3, ..., annotation_30],\n",
        "    \"harry\": [annotation_1, annotation_2, ..., annotation_28],\n",
        "    \"ron\": [annotation_1, annotation_2, ..., annotation_29] \n",
        "}\n",
        "```\n",
        "where each `annotation_i` is one of the JSONL records that you exported, e.g. `{\"id\": 1, \"data\": \"an accident happened...\", \"label\": \"injury\", ... }`. The\n",
        "output should look like this:\n",
        "```python\n",
        "{\n",
        "    1: {\"hermione\": annotation, \"harry\": annotation, \"ron\": annotation},\n",
        "    ...,\n",
        "    29: {\"hermione\": annotation, \"ron\": annotation},\n",
        "    30: {\"hermione\": annotation}\n",
        "}\n",
        "```\n",
        "3. A function that counts how many annotations were made by each annotator. The output should look like this:\n",
        "```python\n",
        "{\n",
        "    \"hermione\": 30,\n",
        "    \"ron\": 29,\n",
        "    \"harry\": 28\n",
        "}\n",
        "```\n",
        "\n",
        "\n",
        "*) N.B.: in a real-world situation, depending on how large the annotation files are and how much resources (memory) you have available, storing a complete JSONL file in memory at the same time might not always be a good idea, but here the quantities of data are such that this isn't a problem\n"
      ],
      "metadata": {
        "id": "KxJ6pvZNzOhT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Write your functions in the cells below. "
      ],
      "metadata": {
        "id": "hisM1OoaQYpO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# function 1\n",
        "import json\n",
        "\n",
        "def read_annotations(anno_file):\n",
        "  records = []\n",
        "  with open(anno_file, encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "      record = json.loads(line)\n",
        "      records.append(record)\n",
        "  return records"
      ],
      "metadata": {
        "id": "kZPaCnrtzh_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function 2\n",
        "def group_annotations(annotator_to_records):\n",
        "  grouped_records = {}\n",
        "  for annotator, records in annotator_to_records.items():\n",
        "    for rec in records:\n",
        "      doc_id = rec[\"article_id\"]\n",
        "      if doc_id in grouped_records:\n",
        "        grouped_records[doc_id][annotator] = rec\n",
        "      else:\n",
        "        grouped_records[doc_id] = {annotator: rec}\n",
        "  return grouped_records"
      ],
      "metadata": {
        "id": "fs8A7kNpzjCZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function 3\n",
        "def count_records(grouped_records):\n",
        "  counts = {}\n",
        "  for doc_id, group in grouped_records.items():\n",
        "    for annotator, record in group.items():\n",
        "      if annotator not in counts:\n",
        "        counts[annotator] = 0\n",
        "      counts[annotator] += 1\n",
        "  return counts\n"
      ],
      "metadata": {
        "id": "AYlHlh84zkHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Use the functions that you wrote to calculate how many annotations every annotator made. In >this< cell, write a short report on your findings: are there large differences? (N.B.: again, this is not a competition of who annotates more! Faster is not always better, and also not vice versa. But it can be interesting to reflect on why: did some annotators need more time per item? For span-based tasks, did some annotators annotate more spans per document? Were there specific issues that only some annotators run into?) "
      ],
      "metadata": {
        "id": "oNm_AzVlQm4J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# calculations\n",
        "annotations = {\n",
        "    \"dertje\": read_annotations(\"annotations/dertje.jsonl\"),\n",
        "    \"justin\": read_annotations(\"annotations/justin.jsonl\"),\n",
        "    \"tariq\": read_annotations(\"annotations/tariq.jsonl\")\n",
        "    # \"maurice\": read_annotations(\"annotations/maurice.jsonl\"),\n",
        "    # \"stijn\": read_annotations(\"annotations/stijn.jsonl\"),\n",
        "\n",
        "}\n",
        "grouped = group_annotations(annotations)\n",
        "print(grouped.keys())\n",
        "counts = count_records(grouped)\n",
        "print(counts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bvm9-K6PQqva",
        "outputId": "4533b6dd-e5f3-4799-9a50-ef750c808358"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys([6046, 8199, 3409, 1851, 1055, 9629, 6434, 8616, 4457, 304, 3472, 5413, 4790, 4552, 11831, 9767, 8793, 4188, 471, 9676, 11593, 8253])\n",
            "{'dertje': 22, 'justin': 2, 'tariq': 2}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Finally, in the cell below, write another function that filters the grouped annotations (from function 2) so that you keep only the ones that were annotated by all annotators."
      ],
      "metadata": {
        "id": "tXXcowRiVPf5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# function 4\n",
        "def get_shared_annotations(grouped_annotations, num_annotators):\n",
        "  shared_annotations = {}\n",
        "  for doc_id, records in grouped_annotations.items():\n",
        "    if len(records)  == num_annotators:\n",
        "      shared_annotations[doc_id] = records\n",
        "  return shared_annotations\n",
        "  \n",
        "shared = get_shared_annotations(grouped, 3)\n",
        "shared"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LEhi-8U8Vp5n",
        "outputId": "620dc766-d298-409b-eb09-57c62d8328b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{6046: {'dertje': {'article_id': 6046,\n",
              "   'crash_id': 5129,\n",
              "   'data': 'Apeldoornse wielrenner (84) overleed vijf dagen na ongeluk op bedrijventerrein\\n\\nDe wielrenner die op 26 juli ernstig gewond raakte bij een ongeval op bedrijventerrein Ecofactorij in Apeldoorn, blijkt te zijn overleden. Dat heeft de politie vandaag bevestigd. Het gaat om een 84-jarige Apeldoorner.\\n\\n',\n",
              "   'id': 771778,\n",
              "   'label': [[12, 22, 'Cyclist'], [83, 93, 'Cyclist']],\n",
              "   'sitename': 'destentor.nl',\n",
              "   'url': 'https://www.destentor.nl/apeldoorn/apeldoornse-wielrenner-84-overleed-vijf-dagen-na-ongeluk-op-bedrijventerrein~abe70587/'},\n",
              "  'justin': {'article_id': 6046,\n",
              "   'crash_id': 5129,\n",
              "   'data': 'Apeldoornse wielrenner (84) overleed vijf dagen na ongeluk op bedrijventerrein\\n\\nDe wielrenner die op 26 juli ernstig gewond raakte bij een ongeval op bedrijventerrein Ecofactorij in Apeldoorn, blijkt te zijn overleden. Dat heeft de politie vandaag bevestigd. Het gaat om een 84-jarige Apeldoorner.\\n\\n',\n",
              "   'id': 763361,\n",
              "   'label': [[0, 27, 'Bicycle'], [83, 93, 'Bicycle']],\n",
              "   'sitename': 'destentor.nl',\n",
              "   'url': 'https://www.destentor.nl/apeldoorn/apeldoornse-wielrenner-84-overleed-vijf-dagen-na-ongeluk-op-bedrijventerrein~abe70587/'},\n",
              "  'tariq': {'article_id': 6046,\n",
              "   'crash_id': 5129,\n",
              "   'data': 'Apeldoornse wielrenner (84) overleed vijf dagen na ongeluk op bedrijventerrein\\n\\nDe wielrenner die op 26 juli ernstig gewond raakte bij een ongeval op bedrijventerrein Ecofactorij in Apeldoorn, blijkt te zijn overleden. Dat heeft de politie vandaag bevestigd. Het gaat om een 84-jarige Apeldoorner.\\n\\n',\n",
              "   'id': 873594,\n",
              "   'label': [[12, 22, 'Cyclist'], [83, 93, 'Cyclist']],\n",
              "   'sitename': 'destentor.nl',\n",
              "   'url': 'https://www.destentor.nl/apeldoorn/apeldoornse-wielrenner-84-overleed-vijf-dagen-na-ongeluk-op-bedrijventerrein~abe70587/'}},\n",
              " 8199: {'dertje': {'article_id': 8199,\n",
              "   'crash_id': 7211,\n",
              "   'data': 'Bestelbus rijdt door na botsing met fietser in centrum Apeldoorn\\n\\nDe politie in Apeldoorn is op zoek naar de bestuurder van een bestelbusje die maandagavond een fietser aanreed in de Hoofdstraat. In plaats van zich om de man te bekommeren, reed hij of zij door.\\n\\nDe politie in Apeldoorn is op zoek naar de bestuurder van een bestelbusje die maandagavond een fietser aanreed in de Hoofdstraat. In plaats van zich om de man te bekommeren, reed hij of zij door. \\n\\nDe botsing vond plaats tegen half negen. De fietser, op weg van werk naar huis, reed over de Hoofdstraat en wilde afslaan naar de Kalverstraat. Het busje verleende geen voorrang en tikte hem aan. De man kwam ten val, maar liep voor zover bekend geen verwondingen op.  Hij werd onderzocht door ambulancepersoneel. Zijn fiets raakte beschadigd.\\n\\nGetuigen\\nDe politie spreekt met de fietser en andere getuigen hoopt een duidelijke beschrijving van het busje en de bestuurder te krijgen. Vorige week vond er ook al een aanrijding plaats in de Hoofdstraat in het centrum van Apeldoorn.',\n",
              "   'id': 771779,\n",
              "   'label': [[0, 9, 'Van'],\n",
              "    [36, 43, 'Cyclist'],\n",
              "    [161, 167, 'Cyclist'],\n",
              "    [358, 365, 'Cyclist'],\n",
              "    [505, 512, 'Cyclist'],\n",
              "    [128, 139, 'Van'],\n",
              "    [325, 336, 'Van'],\n",
              "    [609, 614, 'Van'],\n",
              "    [779, 784, 'Cyclist'],\n",
              "    [840, 847, 'Cyclist'],\n",
              "    [909, 914, 'Van']],\n",
              "   'sitename': 'www.ad.nl',\n",
              "   'url': 'https://www.ad.nl/apeldoorn/bestelbus-rijdt-door-na-botsing-met-fietser-in-centrum-apeldoorn~ac484a6e0/'},\n",
              "  'justin': {'article_id': 8199,\n",
              "   'crash_id': 7211,\n",
              "   'data': 'Bestelbus rijdt door na botsing met fietser in centrum Apeldoorn\\n\\nDe politie in Apeldoorn is op zoek naar de bestuurder van een bestelbusje die maandagavond een fietser aanreed in de Hoofdstraat. In plaats van zich om de man te bekommeren, reed hij of zij door.\\n\\nDe politie in Apeldoorn is op zoek naar de bestuurder van een bestelbusje die maandagavond een fietser aanreed in de Hoofdstraat. In plaats van zich om de man te bekommeren, reed hij of zij door. \\n\\nDe botsing vond plaats tegen half negen. De fietser, op weg van werk naar huis, reed over de Hoofdstraat en wilde afslaan naar de Kalverstraat. Het busje verleende geen voorrang en tikte hem aan. De man kwam ten val, maar liep voor zover bekend geen verwondingen op.  Hij werd onderzocht door ambulancepersoneel. Zijn fiets raakte beschadigd.\\n\\nGetuigen\\nDe politie spreekt met de fietser en andere getuigen hoopt een duidelijke beschrijving van het busje en de bestuurder te krijgen. Vorige week vond er ook al een aanrijding plaats in de Hoofdstraat in het centrum van Apeldoorn.',\n",
              "   'id': 763362,\n",
              "   'label': [[0, 10, 'Van'],\n",
              "    [36, 43, 'Bicycle'],\n",
              "    [161, 169, 'Bicycle'],\n",
              "    [128, 139, 'Van'],\n",
              "    [324, 336, 'Van'],\n",
              "    [357, 366, 'Bicycle'],\n",
              "    [505, 512, 'Bicycle'],\n",
              "    [609, 614, 'Van'],\n",
              "    [779, 784, 'Bicycle'],\n",
              "    [840, 847, 'Bicycle'],\n",
              "    [909, 914, 'Van'],\n",
              "    [245, 249, 'Car']],\n",
              "   'sitename': 'www.ad.nl',\n",
              "   'url': 'https://www.ad.nl/apeldoorn/bestelbus-rijdt-door-na-botsing-met-fietser-in-centrum-apeldoorn~ac484a6e0/'},\n",
              "  'tariq': {'article_id': 8199,\n",
              "   'crash_id': 7211,\n",
              "   'data': 'Bestelbus rijdt door na botsing met fietser in centrum Apeldoorn\\n\\nDe politie in Apeldoorn is op zoek naar de bestuurder van een bestelbusje die maandagavond een fietser aanreed in de Hoofdstraat. In plaats van zich om de man te bekommeren, reed hij of zij door.\\n\\nDe politie in Apeldoorn is op zoek naar de bestuurder van een bestelbusje die maandagavond een fietser aanreed in de Hoofdstraat. In plaats van zich om de man te bekommeren, reed hij of zij door. \\n\\nDe botsing vond plaats tegen half negen. De fietser, op weg van werk naar huis, reed over de Hoofdstraat en wilde afslaan naar de Kalverstraat. Het busje verleende geen voorrang en tikte hem aan. De man kwam ten val, maar liep voor zover bekend geen verwondingen op.  Hij werd onderzocht door ambulancepersoneel. Zijn fiets raakte beschadigd.\\n\\nGetuigen\\nDe politie spreekt met de fietser en andere getuigen hoopt een duidelijke beschrijving van het busje en de bestuurder te krijgen. Vorige week vond er ook al een aanrijding plaats in de Hoofdstraat in het centrum van Apeldoorn.',\n",
              "   'id': 873595,\n",
              "   'label': [[0, 9, 'Delivery_vehicle'],\n",
              "    [36, 43, 'Cyclist'],\n",
              "    [128, 139, 'Delivery_vehicle'],\n",
              "    [161, 168, 'Cyclist']],\n",
              "   'sitename': 'www.ad.nl',\n",
              "   'url': 'https://www.ad.nl/apeldoorn/bestelbus-rijdt-door-na-botsing-met-fietser-in-centrum-apeldoorn~ac484a6e0/'}}}"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Finding (dis)agreements\n",
        "\n",
        "* Now it's time to process your annotations in such a way that you can calculate agreement over them. How this should be done depends on the type of problem you have chosen and how you have approached the problem:\n",
        "\n",
        "  * **Document-level tasks, single label** --- if each document can have only one label, then agreement is simple: two annotators agree on document $X$ if and only if they assigned the same label to $X$\n",
        "  * **Document-level tasks, multiple labels** --- if two annotators assign exactly the same set of tags to document $X$, then there is _exact agreement_, if some but not all of the tags match, there is _partial agreement_, and otherwise there is no agreement.\n",
        "  * **Span-level tasks** --- if two annotators assign the same tag to exactly the same span of text (e.g. the span \"ambulance\" was tagged as `emergency_vehicle`), then there is _exact agreement_; if they assign the same tag to partially the same span (e.g. one annotator has \"the ambulance\" and the other has just \"ambulance\"), there is _partial agreement_, and otherwise there is no agreement. \n",
        "\n",
        "* Start from the output of function 4 that you wrote above. This should be a dictionary of the form `{doc_id: { \"hermione\": {...}, \"harry\": {...}, \"ron\": {...} }, ...}`\n",
        "* From this, we want to create a dictionary like this:\n",
        "```python\n",
        "{\n",
        "    \"hermione\": [\"pedestrian\", \"cyclist\", \"emergency_vehicle\", ... ],\n",
        "    \"harry\": [\"pedestrian\", \"pedestrian\", \"emergency_vehicle\", ... ],\n",
        "    \"ron\": [\"cyclist\", \"cyclist\", \"emergency_vehicle\", ... ],\n",
        "}\n",
        "```\n",
        "i.e., a dictionary mapping annotator names to lists of strings labels. (This means that we get rid of all other information such as document IDs; we want to get a data into such a format that we can directly compute an agreement metric over it)\n",
        "* You need to write the function to create such a dictionary by yourself, but we'll give you instructions for how to approach this. Below, follow **only** the instructions from the section that applies to your situation.\n"
      ],
      "metadata": {
        "id": "N0_ykgtRZGuY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Document-level, single label\n",
        "\n",
        "Follow these instructions if you chose a document-level task and annotated in such a way that there is never more than one label per document\n",
        "\n",
        "* Take the output of function 4\n",
        "* Create a list for every annotator\n",
        "* Loop over the documents, and for every document, add each annotator's label to that annotator's list"
      ],
      "metadata": {
        "id": "xGAu-nh8eoh_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# write your code here\n",
        "def process_doc_sing(shared_annotations, annotator_names):\n",
        "  result = {}\n",
        "  for an in annotator_names:\n",
        "    result[an] = []\n",
        "  \n",
        "  for doc_id, records in shared_annotations.items():\n",
        "    for an, rec in records.items():\n",
        "      label = rec[\"label\"][0]\n",
        "      result[an].append(label)\n",
        "  return result"
      ],
      "metadata": {
        "id": "0Scuh8S4fn65"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Document-level, multiple labels\n",
        "\n",
        "You will need to write two functions for preparing the data:\n",
        "\n",
        "1. Preparing for exact matches\n",
        "  * Take the output of function 4\n",
        "  * Create a list for every annotator\n",
        "  * Loop over the documents, and for every document, do the following:\n",
        "    * For each annotator, sort list the labels (the labels should match, the order doesn't matter), e.g. `[\"injured\", \"dead\", \"damage\"]` --> `\"[damage\", \"dead\", \"injured\"]`\n",
        "    * Make a single string from the sorted labels, e.g. `\"damage,dead,injured\"` (the separator doesn't matter, as long as it is possible to separate the labels again later on, so don't choose a separator that can also occur inside a label, e.g. `_` if you have labels like `\"emergency_vehicle\"`)\n",
        "    * Add the combined label to the annotator's list\n",
        "    * Essentially, what we do here is that we treat multiple labels as if they were a single label\n",
        "2. Preparing for partial matches\n",
        "  * Take the output of function 4\n",
        "  * Create a list for every annotator\n",
        "  * Loop over the documents, and for every document, do the following:\n",
        "    * Compare the label sets of the different annotators\n",
        "    * If there is a label that is shared between all annotators, choose that label for all annotators. (E.g. if Hermione had labels `[A, B]`, Ron had labels `[B, C]`, and Harry had labels `[B, D]`, then we add `B` to each annotator's list)\n",
        "    * If no label is shared between all annotators, add an empty label (e.g., `\"NULL\"`)\n"
      ],
      "metadata": {
        "id": "Inva6Xl_hGJc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# write your exact match code here\n",
        "def process_doc_multi_exact(shared_annotations, annotator_names):\n",
        "  result = {}\n",
        "  for an in annotator_names:\n",
        "    result[an] = []\n",
        "  \n",
        "  for doc_id, records in shared_annotations.items():\n",
        "    for an, rec in records.items():\n",
        "      label = \"++\".join(sorted(rec[\"label\"]))\n",
        "      result[an].append(label)\n",
        "  return result"
      ],
      "metadata": {
        "id": "qxBiSJPUkuEP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# write your partial match code here\n",
        "def process_doc_multi_partial(shared_annotations, annotator_names):\n",
        "  result = {}\n",
        "  for an in annotator_names:\n",
        "    result[an] = []\n",
        "  \n",
        "  for doc_id, records in shared_annotations.items():\n",
        "    shared_labels = None\n",
        "    for an, rec in records.items():\n",
        "      labels = set(rec[\"label\"])\n",
        "      if shared_labels is None:\n",
        "        shared_labels = labels\n",
        "      else:\n",
        "        shared_labels = labels.intersection(shared_labels)\n",
        "\n",
        "    label = \"++\".join(sorted(shared_labels))\n",
        "    for an in annotator_names:\n",
        "      result[an].append(label)\n",
        "      \n",
        "  return result"
      ],
      "metadata": {
        "id": "0Tm9X1pwWwki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Span-level, multiple labels\n",
        "\n",
        "You will need to write two functions for preparing the data:\n",
        "\n",
        "1. Preparing for exact matches\n",
        "  * Take the output of function 4\n",
        "  * Create a list for every annotator\n",
        "  * Loop over the documents, and for every document, do the following:\n",
        "    * Loop over annotators \n",
        "    * Loop over all tagged spans\n",
        "    * For every tagged span, search in the other annotators' tags if they annotated exactly the same span. For all annotators that tagged the same span with the same label, add the label to the list. For all annotators, add an empty label to the list. (E.g.: if Hermione and Harry, but not Ron, applied the tag `\"vehicle\"` to span `[20, 25]`, add `\"vehicle\"` to Hermione and Harry's lists, and add `\"NULL\"` to Ron's list)\n",
        "\n",
        "2. Preparing for partial matches\n",
        "  * Take the output of function 4\n",
        "  * Create a list for every annotator\n",
        "  * Loop over the documents, and for every document, do the following:\n",
        "    * Loop over annotators \n",
        "    * Loop over all tagged spans\n",
        "    * For every tagged span, search in the other annotators' tags if they annotated an overlapping span. For all annotators that tagged an overlapping span with the same label, add the label to the list. For all annotators, add an empty label to the list. (E.g.: if Hermione applied the tag `\"vehicle\"` to span `[20, 25]`, while Harry added `\"vehicle\"` to span `[17, 25]` and Ron completely missed this span, add `\"vehicle\"` to Hermione and Harry's lists, and add `\"NULL\"` to Ron's list) \n",
        "    * Count spans as \"overlapping\" only if one of the spans includes (is a superstring of) the other. I.e., if span A is `[X_i, X_j]` and span B is `[Y_i, Y_j]`, it should be the case that `(X_i <= Y_i and X_j >= Y_j) or (X_i >= Y_i and X_j <= Y_j)`. So, for example A = \"the red ambulance\" and B = \"ambulance\" counts, but A = \"the red\" and B = \"red ambulance\" doesn't count)"
      ],
      "metadata": {
        "id": "REZB5BoHZCE-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# write your exact match code here\n",
        "def process_span_exact(shared_annotations, annotator_names):\n",
        "  result = {}\n",
        "  for an in annotator_names:\n",
        "    result[an] = []\n",
        "  \n",
        "  for doc_id, records in shared_annotations.items():\n",
        "    # make sure we never match twice\n",
        "    already_matched = {}\n",
        "    for an in annotator_names:\n",
        "      already_matched[an] = []\n",
        "\n",
        "    for an_i in annotator_names:\n",
        "      an_i_labels = [tuple(i) for i in records[an_i][\"label\"]]\n",
        "      for lab in an_i_labels:\n",
        "        if lab in already_matched[an_i]:\n",
        "          continue\n",
        "        span_start, span_end, tag_name = lab\n",
        "        result[an_i].append(tag_name)\n",
        "        for an_j in annotator_names:\n",
        "          if an_i == an_j:\n",
        "            continue\n",
        "          an_j_labels = [tuple(i) for i in records[an_j][\"label\"]]\n",
        "          if lab in an_j_labels:\n",
        "            result[an_j].append(tag_name)\n",
        "            # remove the label so that we won't match it again later in the loop\n",
        "            already_matched[an_j].append(lab)\n",
        "          else:\n",
        "            result[an_j].append(\"NULL\")\n",
        "           \n",
        "  return result"
      ],
      "metadata": {
        "id": "jB25jga_fosT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# write your partial match code here\n",
        "def process_span_partial(shared_annotations, annotator_names):\n",
        "  result = {}\n",
        "  for an in annotator_names:\n",
        "    result[an] = []\n",
        "  \n",
        "  for doc_id, records in shared_annotations.items():\n",
        "    # make sure we never match twice\n",
        "    already_matched = {}\n",
        "    for an in annotator_names:\n",
        "      already_matched[an] = []\n",
        "\n",
        "    for an_i in annotator_names:\n",
        "      an_i_labels = [tuple(i) for i in records[an_i][\"label\"]]\n",
        "      for lab in an_i_labels:\n",
        "        if lab in already_matched[an_i]:\n",
        "          continue\n",
        "        span_start, span_end, tag_name = lab\n",
        "        result[an_i].append(tag_name)\n",
        "        for an_j in annotator_names:\n",
        "          if an_i == an_j:\n",
        "            continue\n",
        "          an_j_labels = [tuple(i) for i in records[an_j][\"label\"]]\n",
        "          partial_match_found = False\n",
        "          for lab_j in an_j_labels:\n",
        "            span_start_j, span_end_j, tag_name_j = lab_j\n",
        "            if tag_name == tag_name_j and ((span_start >= span_start_j and span_end <= span_end_j) or (span_start <= span_start_j and span_end >= span_end_j)):\n",
        "              # remove the label so that we won't match it again later in the loop\n",
        "              already_matched[an_j].append(lab_j)\n",
        "              partial_match_found = True\n",
        "          if partial_match_found:\n",
        "            result[an_j].append(tag_name)\n",
        "          else:\n",
        "            result[an_j].append(\"NULL\")  \n",
        "          \n",
        "  return result"
      ],
      "metadata": {
        "id": "QUq8FZJSpOO1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "span_exact = process_span_exact(shared, [\"dertje\", \"justin\", \"tariq\"])"
      ],
      "metadata": {
        "id": "vB96Kk1gil6r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "span_partial = process_span_partial(shared, [\"dertje\", \"justin\", \"tariq\"])"
      ],
      "metadata": {
        "id": "4XXJ_uZlp1dG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calculating Cohen's $\\kappa$\n",
        "\n",
        "* Now it's finally time to actually calculate the agreement!\n",
        "* We'll first do Cohen's $\\kappa$. If your group had 2 annotators, you'll only have to do this once. If there were more than 2 annotators, repeat the procedure for all pairs (e.g. if the annotators are Hermione, Harry & Ron --> repeat for Hermione & Harry, Hermione & Ron, and Harry & Ron)\n",
        "* Scikit-Learn makes it really easy to compute the metric. See the documentation here: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.cohen_kappa_score.html"
      ],
      "metadata": {
        "id": "vxQakU80t_XH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# complete the code in the cell and run it\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "\n",
        "print(cohen_kappa_score(span_exact[\"dertje\"], span_exact[\"tariq\"]))\n",
        "print(cohen_kappa_score(span_partial[\"dertje\"], span_partial[\"tariq\"]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xUqKeb7SunqB",
        "outputId": "fb8b13f3-1120-4fb0-88ec-d15cbb818db5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.12903225806451624\n",
            "0.23699421965317913\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Report on your the results: how much agreement do you get? Is it higher or lower than you expected? If applicable, how much difference is there between exact and partial matches? If there were multiple pairs, which ones had better agreement? Did you expect this?"
      ],
      "metadata": {
        "id": "qMyfuI_7up0g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calculating Fleiss' $\\kappa$\n",
        "\n",
        "* Now it's time for Fleiss! Unfortunately, this isn't included in Scikit-Learn, but there is an alternative package: `statsmodels`. You can find the documentation here: https://www.statsmodels.org/stable/generated/statsmodels.stats.inter_rater.fleiss_kappa.html\n",
        "* The function below prepares your data: we need to make a table (matrix) where the rows are samples and the columns are category distributions"
      ],
      "metadata": {
        "id": "Aopj8z8hrCEg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install statsmodels\n",
        "from statsmodels.stats import inter_rater as irr\n",
        "\n",
        "def prepare_fleiss_table(annotators_to_label_lists):\n",
        "  table = []\n",
        "  for row in zip(*annotators_to_label_lists.values()):\n",
        "    table.append(row)\n",
        "  return irr.aggregate_raters(table)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TkoE17tVrPpI",
        "outputId": "9556cfde-48d9-4f8b-901c-14ec2cfb7e5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: statsmodels in /usr/local/lib/python3.7/dist-packages (0.10.2)\n",
            "Requirement already satisfied: scipy>=0.18 in /usr/local/lib/python3.7/dist-packages (from statsmodels) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from statsmodels) (1.21.6)\n",
            "Requirement already satisfied: pandas>=0.19 in /usr/local/lib/python3.7/dist-packages (from statsmodels) (1.3.5)\n",
            "Requirement already satisfied: patsy>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from statsmodels) (0.5.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.19->statsmodels) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.19->statsmodels) (2022.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from patsy>=0.4.0->statsmodels) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# adapt / complete the code below\n",
        "aggregate_exact = prepare_fleiss_table(span_exact)[0]\n",
        "aggregate_partial = prepare_fleiss_table(span_partial)[0]\n",
        "\n",
        "print(irr.fleiss_kappa(aggregate_exact))\n",
        "print(irr.fleiss_kappa(aggregate_partial))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JcyYKOELsrFx",
        "outputId": "6362085b-2939-435a-ecd3-abb3a96c2300"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-0.10902636916835681\n",
            "-0.03164179104477613\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Again, report on your the results: how much agreement do you get? Is it higher or lower than you expected? If applicable, how much difference is there between exact and partial matches? Are the results for Fleiss' $\\kappa$ very different from those for Cohen's $\\kappa$?"
      ],
      "metadata": {
        "id": "LR-6qHRsxaKy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stage 1.2/A: Qualitative Analysis\n",
        "\n",
        "* Make a shared spreadsheet (Google Sheets or similar)\n",
        "* Go through *at least 25%* of your labeled documents and identify all cases of disagreement\n",
        "* You can either manually browse the Doccano projects and compare the differences, or, if you think this makes the task easier, you can write some code that automatically finds differences (if you do this, you could take inspiration from the researchers' work -- see above)\n",
        "* For each disagreement that you find, add a row to the spreadhseet with the following columns:\n",
        "  * **WHERE?**: in which document?\n",
        "  * **WHO?**: which annotators disagree here?\n",
        "  * **WHAT?**: what is the difference?\n",
        "  * **TYPE**: what type of disagreement? \n",
        "    * for document-level annotations: \"partial\" (only some labels are different) or \"full\" (none of the labels match)\n",
        "    * for span-level annotations: \"same span, different label\", \"partially matching span, same label\", \"missing\" (= neither span, nor label matches)\n",
        "  * **REASON**: try to find a reason for the disagreement. You can label the reasons in any way that you want, but some possible labels are \"guidelines\" (something was ambiguous in the guidelines), \"annotator error\" (the annotator misinterpreted the guidelines), or \"unknown\" (if you can't find out what the cause was)"
      ],
      "metadata": {
        "id": "MvgKlfR4xnx5"
      }
    }
  ],
  "metadata": {
    "colab": {
      "name": "Week 4 - Assignment 2 - Part I (Correction Sheet)",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}